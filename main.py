# ‡πÑ‡∏ü‡∏•‡πå main.py

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List
import joblib
import re
from pythainlp.tokenize import word_tokenize
import trafilatura
from concurrent.futures import ThreadPoolExecutor
import json
import sys


# ‡∏™‡∏£‡πâ‡∏≤‡∏á App
app = FastAPI()

# ==========================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI
# ==========================================

# 1. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
def thai_text_processor(text):
    text = re.sub(r'[^\u0E00-\u0E7Fa-zA-Z\s]', '', str(text))
    words = word_tokenize(text, engine='newmm')
    return " ".join(words)

def split_tokenizer(text):
    return text.split()

import __main__
# ‚ùå ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°: setattr(__main__, "split_tokenizer", split_tokenizer)
# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô: ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ "my_tokenizer" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏≠‡∏Å Model ‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°
setattr(__main__, "my_tokenizer", split_tokenizer)

# ==========================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
# ==========================================
print("Loading AI models...")
model = None
vectorizer = None

try:
    model = joblib.load("model_ai.joblib")
    vectorizer = joblib.load("vector.joblib")
    print("‚úÖ AI Models loaded successfully!")
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Model files not found or error loading. ({e})")


# Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ö Input ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡πà‡∏≤‡∏ß
class NewsRequest(BaseModel):
    headline: str
    
@app.post("/predict")
def predict(request: NewsRequest):
    # print("üîÑ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ó‡∏µ‡πà Predict") 
    if not model or not vectorizer:
        raise HTTPException(status_code=500, detail="Model is not loaded.")
    
    try:
        processed_headline = thai_text_processor(request.headline)
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ vectorizer ‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡∏°‡∏≤
        vectorized_headline = vectorizer.transform([processed_headline])
        
        # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠ 1)
        prediction = model.predict(vectorized_headline)[0]
        prob_list = model.predict_proba(vectorized_headline)[0]
        confidence = max(prob_list) * 100
        
        # ‚úÖ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°: ‡πÅ‡∏õ‡∏•‡∏á 0/1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        # (‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô 1 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏•‡∏≠‡∏° ‡πÅ‡∏ó‡∏ô)
        if str(prediction) == "1":
            result_text = "‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏£‡∏¥‡∏á"
        else:
            result_text = "‡∏Ç‡πà‡∏≤‡∏ß‡∏õ‡∏•‡∏≠‡∏°"

        return {
            "prediction": result_text,       # ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
            "confidence": f"{confidence:.2f}",
            "processed_text": processed_headline,
            "raw_value": str(prediction)     # (‡πÅ‡∏ñ‡∏°) ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏î‡∏¥‡∏ö 0/1 ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ debug
        }
    except Exception as e:
        print(f"Error prediction: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==========================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏π‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
# ==========================================
class ScrapeRequest(BaseModel):
    urls: list[str]

def fetch_url_data(url):
    result_data = {
        "link": url,
        "title": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ",
        "description": "No content found",
        "imageUrl": "",
        "content": ""
    }
    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            extracted_json = trafilatura.extract(
                downloaded, include_images=True, include_comments=False, 
                output_format="json", with_metadata=True
            )
            if extracted_json:
                data = json.loads(extracted_json)
                title = data.get("title") or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"
                full_text = data.get("text") or ""
                excerpt = data.get("excerpt") or (full_text[:300] + "..." if full_text else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
                image = data.get("image") or ""
                
                result_data.update({
                    "title": title,
                    "description": excerpt,
                    "imageUrl": image,
                    "content": full_text
                })
    except Exception as e:
        print(f"Error scrapping {url}: {e}")
        result_data["description"] = f"Error: {str(e)}"

    return result_data

def fetch_url_title(url):
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Default ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
    result_data = {
        "title": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ"
    }
    
    try:
        downloaded = trafilatura.fetch_url(url)
        
        if downloaded:
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å .extract ‡πÄ‡∏õ‡πá‡∏ô .bare_extraction
            # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Python Dict ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏õ‡∏•‡∏á JSON
            data = trafilatura.bare_extraction(
                downloaded, 
                with_metadata=True, 
                include_comments=False
            )
            
            if data:
                # ‡∏î‡∏∂‡∏á title ‡∏à‡∏≤‡∏Å Dict ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
                title = data.get("title")
                
                # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡∏ß‡πà‡∏≤ title ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
                if title:
                    result_data["title"] = title
                else:
                    result_data["title"] = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á (‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠)"
            else:
                 # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏´‡∏•‡∏î HTML ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
                 result_data["title"] = "‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (Structure error)"

    except Exception as e:
        print(f"Error scrapping {url}: {e}")
        result_data["title"] = f"Error: {str(e)}"

    return result_data


@app.post("/scrape-news")
def scrape_news(request: ScrapeRequest):
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(fetch_url_data, request.urls))
    return {"array": results}


# ==========================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏£‡∏∞‡∏ö‡∏ö‡∏î‡∏π‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß
# ==========================================
class ScrapeTitleRequest(BaseModel):
    urls: list[str]

@app.post("/scrape-title")
def scrape_title(request: ScrapeTitleRequest):
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(fetch_url_title, request.urls))
    return {"array": results}

@app.get("/")
def health_check():
    return {"status": "API is running", "features": ["predict", "scrape-news", "retrain"]}